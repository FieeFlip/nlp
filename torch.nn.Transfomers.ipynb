{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt) # 没有实现position embedding ，也需要自己实现mask机制。否则不是你想象的transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现mask\n",
    "torch.nn.Transformer的forward函数实现了mask<br/>\n",
    "- Examples: output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    \n",
    "Docstring:\n",
    "Take in and process masked source/target sequences.\n",
    "\n",
    "Args:<br/> \n",
    "    - src: the sequence to the encoder (required).\n",
    "    - tgt: the sequence to the decoder (required).\n",
    "    - src_mask: the additive mask for the src sequence (optional).\n",
    "    - tgt_mask: the additive mask for the tgt sequence (optional).\n",
    "    - memory_mask: the additive mask for the encoder output (optional).\n",
    "    - src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
    "    - tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
    "    - memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
    "\n",
    "Shape:\n",
    "    - src: :math:`(S, N, E)`.\n",
    "    - tgt: :math:`(T, N, E)`.\n",
    "    - src_mask: :math:`(S, S)`.\n",
    "    - tgt_mask: :math:`(T, T)`.\n",
    "    - memory_mask: :math:`(T, S)`.\n",
    "    - src_key_padding_mask: :math:`(N, S)`.\n",
    "    - tgt_key_padding_mask: :math:`(N, T)`.\n",
    "    - memory_key_padding_mask: :math:`(N, S)`.\n",
    "\n",
    "    Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n",
    "    positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "    while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "    are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "    is provided, it will be added to the attention weight. \n",
    "    [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
    "    the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n",
    "    positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "    value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "\n",
    "    - output: :math:`(T, N, E)`.\n",
    "\n",
    "    # decoder的输出与输入一样\n",
    "    Note: Due to the multi-head attention architecture in the transformer model,\n",
    "    the output sequence length of a transformer is same as the input sequence\n",
    "    (i.e. target) length of the decode.\n",
    "\n",
    "    where S is the source sequence length, T is the target sequence length, N is the\n",
    "    batch size, E is the feature number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src指encoder，tgt指decoder，memory指decoder每个block的第二层和encoder做cross attention的时候。<br/>\n",
    "\\*_key_padding_mask: 用来遮蔽<PAD>以避免pad token的embedding输入。<br/>\n",
    "src_mask/tgt_mask/memory_mask: 附加的mask，用来避免指定位置的embedding输入。tgt_mask做生成时可以实现tgt_mask。<br/>\n",
    "    \n",
    "\\*_mask 对应的API是attn_mask，\\*_key_padding_mask对应的API是key_padding_mask(在torch.nn.modules.activation.MultiheadAttention.forward中)<br/>\n",
    "\n",
    "def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored. When given\n",
    "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
    "            layer will be ignored\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
    "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*_key_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子：batch_size是3，sequence_length是4.\n",
    "```python\n",
    "[\n",
    "    [‘a’,'b','c','<PAD>'],\n",
    "    [‘a’,'b','c','d'],\n",
    "    [‘a’,'b','<PAD>','<PAD>']\n",
    "]\n",
    "```\n",
    "如果你做self-attention的计算（可以是encoder，也可以是decoder),并不希望a、b、c等使用到PAD的信息，所以可以使用key_padding_mask遮住它们。<br/>\n",
    "key_padding_mask的小是(batch_size, sequence_length),对应这个例子：\n",
    "```python\n",
    "\n",
    "[\n",
    "    [False, False, False, True],\n",
    "    [False, False, False, False],\n",
    "    [False, False, True, True]\n",
    "]\n",
    "```\n",
    "key_padding_mask本质上是遮住key这个位置的值（置0），但是PAD token本身，也是会做qkv的计算的，以第三行数据的第三个位置为例，它的q是<PAD>的embedding，k和v分别各是第一个的‘a’和第二个的‘b’，它也会输出一个embedding。softmax之后仍然有值。<font color='red'> 所以你的模型训练在transformer最后的output计算loss的时候，还需要指定ignore_index=pad_index(被pad掉的index)。 </font>以第三行数据为例，它的监督信号是[3205,1890,0,0]，pad_index=0 。如此一来，即便位于<PAD>的transformer会疯狂的和有意义的position做qkv，也会输出embedding，但是我们不算它的loss，任凭它各种作妖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1711)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "pred = []\n",
    "pred.append([0.9, 0.1])\n",
    "pred.append([0.8, 0.2])\n",
    "pred = torch.Tensor(pred).view(-1,  2)\n",
    "\n",
    "# 这里输出类别为0或1，-1表示不参与计算loss。\n",
    "# 且计算平均loss的时候，reduction只计算实际参与计算的个数，这里相当于batchsize=2，但其中第index=1行为-1不参与计算loss。\n",
    "label = torch.LongTensor([[1], [-1]])  \n",
    "\n",
    "# out = F.cross_entropy(pred.view(-1, 2), label.view(-1, )) \n",
    "out = F.cross_entropy(pred.view(-1, 2), label.view(-1, ), ignore_index=-1) \n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*_mask对应attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是上面那个例子，以第一行数据`['a','b','c','<PAD>']`,为例（假设我们在用decoder做生成，研究<font color='red'>block 的第一层layer </font>也就是self-attention），此时：\n",
    "- 'a'可以看到'a'\n",
    "- 'b'可以看到'a','b'\n",
    "- 'c'可以看到'a','b','c'\n",
    "- `'<PAD>'`理论上不应该看到什么，但是只要它头顶的监督信号是ignore_index，那就没有关系，所以让他看到'a','b','c','<PAD>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2维的时候是（L,S），3维的时候是（N*num_heads, L, S）。此时，由于decoder第一层layer的qkv都是同一个序列，所以L=S。这是个正方形。<br/>\n",
    "torrch.nn.Transformer.generate_square_subsequent_mask实现了此mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "generate_square_subsequent_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "\n",
    "# 1是<PAD>\n",
    "src_key_padding_mask = torch.tensor(np.zeros((32, 10)))\n",
    "src_key_padding_mask[:, -1:] =  1\n",
    "src_key_padding_mask[3:7, -5:-1] = 1\n",
    "\n",
    "tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(transformer_model, 20)\n",
    "\n",
    "out = transformer_model(src, tgt, tgt_mask=tgt_mask, \n",
    "                        src_key_padding_mask = src_key_padding_mask) # 实现了mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
